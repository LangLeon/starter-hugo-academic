---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Modeling Human Beliefs about AI Behavior for Scalable Oversight
subtitle: ''
summary: 'We explain how modeling human evaluator beliefs about AI behavior can help to better interpret their feedback.'
authors:
- Leon Lang
- Patrick Forr√©
tags:
- Computer Science - Artificial Intelligence
categories: []
date: '2025-02-28'
lastmod: 2025-02-28T18:11:46+01:00
featured: false
draft: false

slides:
- example

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2025-02-28T17:11:46.691694Z'
publication_types:
- '3'
abstract: "Contemporary work in AI alignment often relies on human feedback to teach AI systems human preferences and values. Yet as AI systems grow more capable, human feedback becomes increasingly unreliable. This raises the problem of scalable oversight: How can we supervise AI systems that exceed human capabilities? In this work, we propose to model the human evaluator's beliefs about the AI system's behavior to better interpret the human's feedback. We formalize human belief models and theoretically analyze their role in inferring human values. We then characterize the remaining ambiguity in this inference and conditions for which the ambiguity disappears. To mitigate reliance on exact belief models, we then introduce the relaxation of human belief model covering. Finally, we propose using foundation models to construct covering belief models, providing a new potential approach to scalable oversight.
"
publication: '*arxiv eprints*'
links:
- name: arXiv
  url: https://www.arxiv.org/abs/2502.21262
---
