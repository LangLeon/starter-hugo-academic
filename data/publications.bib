@inproceedings{Lang2021a,
abstract = {Group equivariant convolutional networks (GCNNs) endow classical convolutional networks with additional symmetry priors, which can lead to a considerably improved performance. Recent advances in the theoretical description of GCNNs revealed that such models can generally be understood as performing convolutions with G-steerable kernels, that is, kernels that satisfy an equivariance constraint themselves. While the G-steerability constraint has been derived, it has to date only been solved for specific use cases - a general characterization of G-steerable kernel spaces is still missing. This work provides such a characterization for the practically relevant case of G being any compact group. Our investigation is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical tensor operators, we prove that steerable kernel spaces are fully understood and parameterized in terms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan coefficients, and 3) harmonic basis functions on homogeneous spaces.},
author = {Lang, Leon and Weiler, Maurice},
booktitle = {International Conference on Learning Representations},
file = {:home/leon/Documents/PhD Amsterdam/Literature/a{\_}wigner{\_}eckart{\_}theorem{\_}for{\_}gr.pdf:pdf},
number = {August},
title = {{A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels}},
url = {https://openreview.net/forum?id=ajOrOhQOsYx},
year = {2021}
}
@inproceedings{Cesa2022a,
abstract = {Equivariance is becoming an increasingly popular design choice to build data efficient neural networks by exploiting prior knowledge about the symmetries of the problem at hand. Euclidean steerable CNNs are one of the most common classes of equivariant networks. While the constraints these architectures need to satisfy are understood, no practical method to parametrize them generally has been described so far, with most existing approaches tailored to specific groups or classes of groups. In this work, we generalize the Wigner-Eckart theorem proposed in Lang {\&} Weiler (2021), which characterizes general G-steerable kernel spaces for compact groups G over their homogeneous spaces, to arbitrary G-spaces. This enables us to directly parameterize filters in terms of a band-limited basis on the base space, but also to easily implement steerable CNNs equivariant to a large number of groups. To demonstrate its generality, we instantiate our method on a large variety of isometry groups acting on the Euclidean space R{\^{}}3. Our general framework allows us to build E(3) and SE(3)-steerable CNNs like previous works, but also CNNs with arbitrary G {\textless} O(3)-steerable kernels. For example, we build 3D CNNs equivariant to the symmetries of platonic solids or choose G=SO(2) when working with 3D data having only azimuthal symmetries. We compare these models on 3D shapes and molecular datasets, observing improved performance by matching the model's symmetries to the ones of the data.},
author = {Cesa, Gabriele and Lang, Leon and Weiler, Maurice},
booktitle = {International Conference on Learning Representations},
file = {:home/leon/Documents/PhD Amsterdam/Literature/a{\_}program{\_}to{\_}build{\_}e{\_}n{\_}equivar.pdf:pdf},
number = {2},
title = {{A Program to Build E(N)-Equivariant Steerable CNNs}},
url = {https://openreview.net/forum?id=WE4qe9xlnQw},
volume = {4},
year = {2022}
}
@article{Kolb2019,
abstract = {Previous research into agent communication has shown that a pre-trained guide can speed up the learning process of an imitation learning agent. The guide achieves this by providing the agent with discrete messages in an emerged language about how to solve the task. We extend this one-directional communication by a one-bit communication channel from the learner back to the guide: It is able to ask the guide for help, and we limit the guidance by penalizing the learner for these requests. During training, the agent learns to control this gate based on its current observation. We find that the amount of requested guidance decreases over time and guidance is requested in situations of high uncertainty. We investigate the agent's performance in cases of open and closed gates and discuss potential motives for the observed gating behavior.},
archivePrefix = {arXiv},
arxivId = {1912.05525},
author = {Kolb, Benjamin and Lang, Leon and Bartsch, Henning and Gansekoele, Arwin and Koopmanschap, Raymond and Romor, Leonardo and Speck, David and Mul, Mathijs and Bruni, Elia},
eprint = {1912.05525},
file = {:home/leon/Documents/PhD Amsterdam/Literature/1912.05525.pdf:pdf},
isbn = {9781950737758},
journal = {LANTERN@EMNLP-IJCNLP 2019 - Beyond Vision and LANguage: inTEgrating Real-World kNowledge, Proceedings},
pages = {41--50},
title = {{Learning to request guidance in emergent communication}},
year = {2019}
}
@article{2022Lang,
abstract = {In information theory, one major goal is to find useful functions that summarize the amount of information contained in the interaction of several random variables. Specifically, one can ask how the classical Shannon entropy, mutual information, and higher interaction information functions relate to each other. This is formally answered by Hu's theorem, which is widely known in the form of information diagrams: it relates disjoint unions of shapes in a Venn diagram to summation rules of information functions; this establishes a bridge from set theory to information theory. While a proof of this theorem is known, to date it was not analyzed in detail in what generality it could be established. In this work, we view random variables together with the joint operation as a monoid that acts by conditioning on information functions, and entropy as the unique function satisfying the chain rule of information. This allows us to abstract away from Shannon's theory and to prove a generalization of Hu's theorem, which applies to Shannon entropy of countably infinite discrete random variables, Kolmogorov complexity, Tsallis entropy, (Tsallis) Kullback-Leibler Divergence, cross-entropy, submodular information functions, and the generalization error in machine learning. Our result implies for Chaitin's prefix-free Kolmogorov complexity that the higher-order interaction complexities of all degrees are in expectation close to Shannon interaction information. For well-behaved probability distributions on increasing sequence lengths, this shows that asymptotically, the per-bit expected interaction complexity and information coincide, thus showing a strong bridge between algorithmic and classical information theory.},
archivePrefix = {arXiv},
arxivId = {cs.IT/2202.09393},
author = {Lang, Leon and Baudot, Pierre and Quax, Rick and Forr{\'{e}}, Patrick},
eprint = {2202.09393},
file = {:home/leon/Documents/PhD Amsterdam/Literature/own{\_}paper.pdf:pdf},
journal = {arXiv e-prints},
keywords = {Computer Science - Information Theory},
pages = {arXiv:2202.09393},
primaryClass = {cs.IT},
title = {{Information Decomposition Diagrams Applied beyond Shannon Entropy: A Generalization of Hu's Theorem}},
url = {http://arxiv.org/abs/2202.09393},
year = {2022}
}
